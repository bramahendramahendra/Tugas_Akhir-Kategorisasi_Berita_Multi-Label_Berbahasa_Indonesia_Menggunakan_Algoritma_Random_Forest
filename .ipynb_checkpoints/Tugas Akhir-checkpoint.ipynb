{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import os\n",
    "import re\n",
    "from library.Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from library.Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tag import CRFTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input_Data():\n",
    "    \n",
    "    def Input_Data_CSV(file):\n",
    "        data = pandas.read_csv(file,header=0,encoding = 'unicode_escape')\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dataset = os.getcwd()+'/dummy/dummy_dataset.csv'\n",
    "data_label = os.getcwd()+'/dummy/dummy_label.csv'\n",
    "data_clean = os.getcwd()+'/dummy/dummy_clean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def Baca_Data(self, data): \n",
    "        dropped_data = data.dropna()\n",
    "        results_data = dropped_data.reset_index()\n",
    "        return results_data\n",
    "    \n",
    "\n",
    "    def Outputan_Data(self):\n",
    "        for i in range(0, len(self.data_hasil['berita'])):\n",
    "            print(str(i+1) +'.- ' +self.data_hasil['berita'][i])\n",
    "            print(' ')\n",
    "            print('---------------------------------------------------')\n",
    "            print(' ')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __init__(self, data): #inisialisasi class Output Data\n",
    "        Hasil_data = self.Baca_Data(data)    \n",
    "        self.data_hasil = {}\n",
    "        self.data_hasil['berita'] = Hasil_data.Berita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 803: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 803: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c68aba2172bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput_Data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput_Data_CSV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutputan_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-38ab858cb578>\u001b[0m in \u001b[0;36mInput_Data_CSV\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mInput_Data_CSV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 803: invalid start byte"
     ]
    }
   ],
   "source": [
    "raw_data = Input_Data.Input_Data_CSV(data_dataset)\n",
    "dataset = Dataset(raw_data)\n",
    "output = dataset.Outputan_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    def Baca_Data(self, data): #untuk membaca data \n",
    "        dropped_data = data.dropna()\n",
    "        results_data = dropped_data.reset_index()\n",
    "        return results_data\n",
    "    \n",
    "    def Punctuation_Removal(self, data): #Punctional removal adalah proses penghilangan tanda baca pada teks\n",
    "        Hasil_Punctuation_Removal = re.sub(\"[^a-zA-Z]\", \" \", data)\n",
    "        return Hasil_Punctuation_Removal\n",
    "    \n",
    "    def Case_Folding(self, data): #case folding adalah proses mengubah huruf besar menjadi kecil\n",
    "        Hasil_Case_Folding = data.lower()\n",
    "        return Hasil_Case_Folding\n",
    "    \n",
    "    def StopWords_Removal(self, data, stopword):\n",
    "        StopWords_Removal = stopword.remove(data)\n",
    "        return StopWords_Removal\n",
    "        \n",
    "    def Stemming_Data(self, data, stemmer):\n",
    "        Hasil_Stemming_Data = stemmer.stem(data)\n",
    "        return Hasil_Stemming_Data\n",
    "    \n",
    "    def Proses_Preprocessing(self, data, PR, CF, stopword, SWR, stemmer, SM): #proses prerocessing\n",
    "        Hasil_Proses_Berita = []\n",
    "        for berita in data.Berita:\n",
    "            if(PR == True):\n",
    "                Hasil_Data = self.Punctuation_Removal(berita)\n",
    "            if(CF == True):\n",
    "                Hasil_Data = self.Case_Folding(Hasil_Data)\n",
    "            if(SWR == True):\n",
    "                Hasil_Data = self.StopWords_Removal(Hasil_Data, stopword)\n",
    "            if(SM == True):\n",
    "                Hasil_Data = self.Stemming_Data(Hasil_Data, stemmer)\n",
    "            Hasil_Proses_Berita.append(Hasil_Data)\n",
    "        return Hasil_Proses_Berita\n",
    "    \n",
    "    def Save_Excel(self, berita):\n",
    "        filename = 'dummy_clean'\n",
    "        filepath = os.getcwd()+'/dummy/' +filename +'.csv'\n",
    "        raw_data = {'Berita': berita}\n",
    "        df = pandas.DataFrame(raw_data, columns = ['Berita'])\n",
    "        df.to_csv(filepath, index=False)\n",
    "        return filename\n",
    "    \n",
    "    def Outputan_Data(self):\n",
    "        for i in range(0, len(self.data_hasil['berita'])):\n",
    "            print(str(i+1) +'.- ' +self.data_hasil['berita'][i])\n",
    "            print(' ')\n",
    "            print('---------------------------------------------------')\n",
    "            print(' ')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __init__(self, data, PR, CF, SWR, SM, save): #iniisialisasi data preprocessing\n",
    "        #inisialisasi StopWords\n",
    "        factory_SWR = StopWordRemoverFactory()\n",
    "        stopword = factory_SWR.create_stop_word_remover()\n",
    "        #inisialisasi Stemming\n",
    "        factory_S = StemmerFactory()\n",
    "        stemmer = factory_S.create_stemmer()\n",
    "        #Poses Preprocessing\n",
    "        hasil_preprocessing = self.Proses_Preprocessing(data, PR, CF, stopword, SWR, stemmer, SM)\n",
    "        #menyimpda data ke excel\n",
    "        if(save == True):\n",
    "            self.filepath = self.Save_Excel(hasil_preprocessing)\n",
    "        self.data_hasil = {}\n",
    "        self.data_hasil['berita'] = hasil_preprocessing\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctional Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(raw_data, True, False, False, False, False)\n",
    "output = preprocessing.Outputan_Data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(raw_data, True, True, False, False, False)\n",
    "output = preprocessing.Outputan_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWord Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(raw_data, True, True, True, False, False)\n",
    "output = preprocessing.Outputan_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(raw_data, True, True, True, True, False)\n",
    "output = preprocessing.Outputan_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(raw_data, True, True, True, True, True)\n",
    "output = preprocessing.Outputan_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = Input_Data.Input_Data_CSV(data_clean)\n",
    "dataset = Dataset(raw_data)\n",
    "output = dataset.Outputan_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger():\n",
    "    def Indonesia_Tagger(self, data, ct):\n",
    "        fitur = {}\n",
    "        clean_punct = RegexpTokenizer(r'\\w+') \n",
    "        arr = [clean_punct.tokenize(data.lower())]\n",
    "        hasil = ct.tag_sents(arr)\n",
    "        return hasil\n",
    "    \n",
    "    def Proses_Tagger(self, data, ct): #proses tagger\n",
    "        Hasil_Proses_Berita = []\n",
    "        for berita in data.Berita:\n",
    "            Hasil_Data = self.Indonesia_Tagger(berita, ct)\n",
    "            Hasil_Proses_Berita.append(Hasil_Data)\n",
    "        return Hasil_Proses_Berita\n",
    "\n",
    "    def Outputan_Data(self):\n",
    "        for i in range(0, len(self.data_hasil['berita'])):\n",
    "            print('.- ' +self.data_hasil['berita'][i])\n",
    "            print(' ')\n",
    "            print('---------------------------------------------------')\n",
    "            print(' ')\n",
    "        return None\n",
    "\n",
    "    def __init__(self, data): #iniisialisasi data tagger\n",
    "        ct = CRFTagger()\n",
    "        ct.set_model_file('./library/tagger/indonesian_tagger')\n",
    "        \n",
    "        #Poses tagger\n",
    "        Hasil_Tagger = self.Proses_Tagger(data, ct)\n",
    "        \n",
    "        self.data_hasil = {}\n",
    "        self.data_hasil['berita'] = Hasil_Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Tagger(raw_data)\n",
    "output = Tagger.Outputan_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for berita in data.Berita:\n",
    "    hasil_berita = indonesia_tagger(berita)\n",
    "    fitur_tagger.append(hasil_berita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Proses_Preprocessing(self, data, PR, CF, stopword, SWR, stemmer, SM): #proses prerocessing\n",
    "        Hasil_Proses_Berita = []\n",
    "        for berita in data.Berita:\n",
    "            if(PR == True):\n",
    "                Hasil_Data = self.Punctuation_Removal(berita)\n",
    "            if(CF == True):\n",
    "                Hasil_Data = self.Case_Folding(Hasil_Data)\n",
    "            if(SWR == True):\n",
    "                Hasil_Data = self.StopWords_Removal(Hasil_Data, stopword)\n",
    "            if(SM == True):\n",
    "                Hasil_Data = self.Stemming_Data(Hasil_Data, stemmer)\n",
    "            Hasil_Proses_Berita.append(Hasil_Data)\n",
    "        return Hasil_Proses_Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "ct.set_model_file('./library/tagger/indonesian_tagger')\n",
    "\n",
    "input_path = data_clean      # Baca data csv\n",
    "fitur_alldoc = {}           # fitur untuk seluruh dokumen ()\n",
    "fitur_onedoc = []           # array, isinya sebanyak data yang digunakan. dan setiap isinya mengandung fitur tiap dokumen (TF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_regex_pos(kalimat):\n",
    "    fitur = {}\n",
    "    clean_punct = RegexpTokenizer(r'\\w+') \n",
    "    arr = [clean_punct.tokenize(kalimat.lower())]\n",
    "    hasil = ct.tag_sents(arr)\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitur_perdoc = [] \n",
    "with open (input_path, 'r', encoding = \"ISO-8859-1\") as inputan :\n",
    "    reader = inputan.read().split(\"\\n\")\n",
    "    no = 0\n",
    "    for row in reader:\n",
    "        hasil = tf_regex_pos(row)\n",
    "        fitur_perdoc.append(hasil)\n",
    "        \n",
    "for no_doc, doc in enumerate(fitur_perdoc):\n",
    "    print(doc)\n",
    "    print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf regex post\n",
    "def tf_regex_pos(kalimat):\n",
    "    fitur = {}\n",
    "    clean_punct = RegexpTokenizer(r'\\w+') \n",
    "    arr = [clean_punct.tokenize(kalimat.lower())]\n",
    "    hasil = ct.tag_sents(arr)\n",
    "     politik = ['politik', 'capres', 'cawapres', 'kebijakan', 'presiden', 'wakil presiden', 'pemerintah', 'kementrian',\n",
    "               'politikus', 'menteri', 'partai', 'pemilihan umum', 'pemilu', 'dpr', 'pemkot', 'pemprov', 'kampanye']\n",
    "    ekonomi = ['ekonomi', 'saham', 'pasar saham', 'pendapatan', 'investasi', 'suku bunga', 'bunga', 'tarif pajak', \n",
    "               'keuangan', 'anggaran', 'pajak', 'miskin', 'biaya']\n",
    "    teknologi = ['game', 'platform', 'versi', 'teknologi', 'programer', 'designer', 'jaringan', 'performa', 'prangkat lunak', \n",
    "                 'perangkat keras', 'fitur', 'smartphone', 'rilis', 'devloper', 'upgrade', 'fasilitas']\n",
    "    for i, sentence in enumerate(hasil):\n",
    "        for j, word_tag in enumerate(sentence):\n",
    "            # informasi\n",
    "            if (word_tag[0] in informasi):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(word_tag[0] == 'barang'):\n",
    "                    word_after = sentence[j+1][0]\n",
    "                    if (word_after == 'siapa'):\n",
    "                        insert_dict(word_tag[0]+word_after, fitur)\n",
    "                else:\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # Verb + lah\n",
    "            elif (word_tag[0][-3:] == 'lah' and len(word_tag[0]) > 3): # kata NN yang akhirannya 'lah' (makanlah)\n",
    "                tag_of_word = beri_tag(word_tag[0][:-3])\n",
    "                # print(tag_of_word)\n",
    "                if(word_tag[0] not in lah_in_word): # allah tag SC, al tag JJ\n",
    "                    if(tag_of_word[0][0][1] == 'JJ' or tag_of_word[0][0][1] == 'RB' or tag_of_word[0][0][1] == 'NEG' or tag_of_word[0][0][1] == 'VB'):\n",
    "                        insert_dict(word_tag[0], fitur)\n",
    "            # %anjur% atau %wajib% sebagai VB\n",
    "            elif ('anjur' in word_tag[0] or 'wajib' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1] # ambil tag dari kata itu\n",
    "                if(tag_of_word == 'VB' or tag_of_word == 'NN'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # %hendak% kecuali dia bertag VB(menghendaki)\n",
    "            elif ('hendak' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(tag_of_word != 'VB'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # %sebaik-baik% tag 'RB' dan 'CC'\n",
    "            elif ('sebaikbaik' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(tag_of_word == 'CC' or tag_of_word == 'RB'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # taat dan mentaatiku - NN\n",
    "            elif ('taat' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(tag_of_word == 'NN'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # %larang% sbg VB or NN\n",
    "            elif ('larang' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(tag_of_word == 'VB' or tag_of_word == 'NN'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # %haram% sbg VB, NN -> SC, NN -> VB, NN -> RB, NN -> IN | VB -> haram(NN)\n",
    "            elif ('haram' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(j < len(sentence)-1 and tag_of_word == 'NN'):\n",
    "                    tag_after = sentence[j+1][1]\n",
    "                    if(tag_after == 'SC' or tag_after == 'VB' or tag_after == 'RB' or tag_after == 'IN'):\n",
    "                        insert_dict(word_tag[0], fitur)\n",
    "                elif(j != 0 and tag_of_word == 'NN'):\n",
    "                    tag_before = sentence[j-1][0]\n",
    "                    if(tag_before == 'VB'):\n",
    "                        insert_dict(word_tag[0], fitur)\n",
    "                elif(tag_of_word == 'VB'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "            # %jangan%, %durhaka%, membangkang\n",
    "            elif ('jangan' in word_tag[0] or 'membangkang' in word_tag[0] or 'membangkangku' in word_tag[0] or 'durhaka' in word_tag[0]):\n",
    "                insert_dict(word_tag[0], fitur)\n",
    "            # tidak -> VB\n",
    "            elif ('tidak' in word_tag[0]):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(j < len(sentence)-2):\n",
    "                    # print(len(sentence))\n",
    "                    word_after = sentence[j+1][0]\n",
    "                    tag_after2 = sentence[j+2][1]\n",
    "                    if(word_after == 'pernah' and tag_after2 == 'VB'):\n",
    "                        insert_dict(word_tag[0], fitur)\n",
    "            # me-kan sbg VB\n",
    "            elif(word_tag[0][0:2] == 'me' and word_tag[0][-3:] == 'kan'):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(tag_of_word == 'VB'):\n",
    "                    insert_dict(word_tag[0], fitur)\n",
    "    return fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = './dummy/dummy_clean.csv'\n",
    "fitur_onedoc, fitur_alldoc = load_fitur_postag(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    # main regex postagging\n",
    "def tf_regex_pos(kalimat):\n",
    "    fitur = {}\n",
    "    clean_punct = RegexpTokenizer(r'\\w+') \n",
    "    arr = [clean_punct.tokenize(kalimat.lower())]\n",
    "    hasil = ct.tag_sents(arr)\n",
    "    print (hasil)\n",
    "    politik = ['politik', 'capres', 'cawapres', 'kebijakan', 'presiden', 'wakil presiden', 'pemerintah', 'kementrian',\n",
    "               'politikus', 'menteri', 'partai', 'pemilihan umum', 'pemilu', 'dpr', 'pemkot', 'pemprov', 'kampanye']\n",
    "    ekonomi = ['ekonomi', 'saham', 'pasar saham', 'pendapatan', 'investasi', 'suku bunga', 'bunga', 'tarif pajak', \n",
    "               'keuangan', 'anggaran', 'pajak', 'miskin', 'biaya']\n",
    "    teknologi = ['game', 'platform', 'versi', 'teknologi', 'programer', 'designer', 'jaringan', 'performa', 'prangkat lunak', \n",
    "                 'perangkat keras', 'fitur', 'smartphone', 'rilis', 'devloper', 'upgrade', 'fasilitas']\n",
    "\n",
    "    for i, sentence in enumerate(hasil):\n",
    "        for j, word_tag in enumerate(sentence):\n",
    "            # politik\n",
    "            if (word_tag[0] in ekonomi):\n",
    "                tag_of_word = word_tag[1]\n",
    "                if(word_tag[0] == 'barang'):\n",
    "                    word_after = sentence[j+1][0]\n",
    "                    if (word_after == 'siapa'):\n",
    "                        insert_dict(word_tag[0]+word_after, fitur)\n",
    "                else:\n",
    "            \n",
    "    return fitur\n",
    "\n",
    "# Read Data\n",
    "def load_fitur_postag(path_data):\n",
    "    input_path = path_data      # Baca data csv\n",
    "    fitur_alldoc = {}           # fitur untuk seluruh dokumen ()\n",
    "    fitur_onedoc = []           # array, isinya sebanyak data yang digunakan. dan setiap isinya mengandung fitur tiap dokumen (TF)\n",
    "    with open (input_path, 'r', encoding = \"ISO-8859-1\") as inputan :\n",
    "        reader = inputan.read().split(\"\\n\")\n",
    "        for row in reader:\n",
    "            fitur_perdoc = ReGex.tf_regex_pos(row)         # Tipe dict isinya fitur dan frekuensi per dokumen | katanya udah unik\n",
    "            fitur_onedoc.append(fitur_perdoc)        # dict dimasukin ke array bernama fitur_onedoc\n",
    "            fitur_alldoc = idf_regex_pos(fitur_perdoc, fitur_alldoc) # fitur perdoc dimasukin ke fitur alldoc dalam bentuk dict\n",
    "    return fitur_onedoc, fitur_alldoc\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
